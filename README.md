# ğŸ¤– FinSolve Role-Based Chatbot

A secure, intelligent chatbot powered by **LLMs + Vector Search (RAG)** â€” with **role-based access control (RBAC)** for Finance, HR, Engineering, Marketing, Employees, and C-Level Executives.

---

## ğŸ§© Problem Background

**FinSolve Technologies**,  a leading FinTech company, was experiencing communication delays and fragmented document access across teams like Finance, HR, Marketing, Engineering, and C-Level Executives. These issues led to slower decision-making and operational inefficiencies, as teams lacked a centralized, secure way to access internal knowledge specific to their roles.

---

## ğŸ§  Solution Overview
To address this issue, an internal AI chatbot was developed using Retrieval Augmented Generation (RAG) and Role-Based Access Control (RBAC). It ensures that every user receives accurate, secure, and role-relevant information instantly.

This chatbot solves FinSolve's data access problem using:
- ğŸ§  **RAG (Retrieval-Augmented Generation)** via LLaMA 3 (Ollama)
- ğŸ” **Role-Based Filtering** at the vector search level
- âš¡ **FastAPI + Streamlit** for interactive chat and login
- ğŸ§¾ **Documents** stored per department with metadata

---

## ğŸ‘¥ Role-Based Access Control (RBAC)

| Role               | Permissions                                                                 |
|--------------------|-----------------------------------------------------------------------------|
| C-Level Executives | Full unrestricted access to all documents                                   |
| Finance Team       | Financial reports, expenses, reimbursements                                 |
| Marketing Team     | Campaign performance, customer insights, sales data                         |
| HR Team            | Employee handbook, attendance, leave, payroll                               |
| Engineering Dept.  | System architecture, deployment, CI/CD                                      |
| Employees          | General information (FAQs, company policies, events)                        |

---

## ğŸš€ Features

### ğŸ” Secure Role-Based Search
- Each user sees **only** their permitted data
- C-level users get **unfiltered** access

### ğŸ’¬ Interactive Chat Interface
- Built with **Streamlit**
- Login panel with session persistence
- Typing animation + Chat history
- ğŸ‘ğŸ‘ feedback buttons
- Role access transparency shown in every response

### ğŸ” Context-Aware Retrieval
- Vector DB powered by **Chroma**
- Embeds `.md` files per department with metadata (`role`, `category`)
- Queries run through vector similarity â†’ LLM â†’ Answer


---

## ğŸ›  Tech Stack

| Layer         | Tool/Library             |
|---------------|--------------------------|
| Frontend      | Streamlit                |
| Backend       | FastAPI                  |
| Embeddings    | SentenceTransformers     |
| Vector DB     | ChromaDB                 |
| LLM           | LLaMA 3 (via Ollama)     |
| Doc Format    | Markdown (.md)           |

---

## ğŸ§ª Sample Users & Roles

| Username | Password     | Role              |
|----------|--------------|-------------------|
| Alice    | ceopass      | c-levelexecutives |
| Bob      | employeepass | employee          |
| Tony     | password123  | engineering       |
| Bruce    | securepass   | marketing         |
| Sam      | financepass  | finance           |
| Natasha  | hrpass123    | hr                |

---

## âš™ï¸ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/sakshcc/role_based_aichatbot
cd role-based-chatbot
```

#### 2. ğŸ”§ Backend Setup (FastAPI + LLaMA 3 via Ollama)

Step into the backend app and create a virtual environment:

```bash
cd app
python -m venv venv
````

Activate the virtual environment:

```bash
venv\Scripts\activate     # On Windows
# OR
source venv/bin/activate  # On Mac/Linux
````

Install the dependencies:

```bash
pip install -r ../requirements.txt
```

In a new terminal, start the LLaMA 3 model using Ollama:

```bash
ollama run llama3
```
ğŸ“ Keep this terminal open â€” it runs the local LLM engine.
The first run will download the model (~3â€“4 GB).

Go back to the backend terminal and start the FastAPI server:

```bash
uvicorn main:app --reload
```

ğŸ”— Visit: http://127.0.0.1:8000

### 3. ğŸ’¬ Frontend Setup (Streamlit UI)
In another new terminal:

```bash
cd frontend
streamlit run frontend.py
```
ğŸ”— Visit: http://localhost:8501

### 4. ğŸ“„ Embed Documents (Run Once Before Use)
To embed documents into ChromaDB:

```bash
python embed_documents.py
```

This script:
Loads documents from the data/ folder
Generates embeddings using sentence-transformers
Stores them in ChromaDB with role-based metadata

âœ… Once these steps are done, your role-based chatbot is fully set up and ready to use! 
